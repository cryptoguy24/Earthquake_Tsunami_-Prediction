{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a82da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bc6d80-8395-46b7-a1cd-db5b74501e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = \"earthquake_tsunami_XGBClassifier.pkl\"\n",
    "PIPELINE_FILE = \"pipeline.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earthquake_tsunami_data_pipeline(train_data: str, valid_data: str, test_data: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates features and labels from test Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    csv: Input data containing earthquake features.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with original data and predictions.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_data = pd.read_csv(train_data)\n",
    "    valid_data = pd.read_csv(valid_data)\n",
    "    testing_dataset = pd.read_csv(test_data)\n",
    "    \n",
    "    # Combine training and validation data to train the model\n",
    "    training_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    #-----------------------------\n",
    "    # For training data\n",
    "    training_features = training_data.drop(columns=[\"Tsunami\"])\n",
    "    training_labels = training_data[\"Tsunami\"].copy()\n",
    "    \n",
    "    # For testing data\n",
    "    testing_features = testing_dataset.drop(columns=[\"Tsunami\"])\n",
    "    testing_labels = testing_dataset[\"Tsunami\"].copy()\n",
    "    \n",
    "    return training_features, training_labels, testing_features, testing_labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e4d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_features, train_labels, test_features, test_labels  = earthquake_tsunami_data_pipeline('../data/processed/train_set.csv',\n",
    "                                                                                             '../data/processed/valid_set.csv',\n",
    "                                                                                             '../data/processed/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1503c99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making models folder in dir to save model\n",
    "if not os.path.exists('../models/'):\n",
    "    os.makedirs('../models/')\n",
    "\n",
    "# Save the trained model\n",
    "if not os.path.exists(f'../models/{MODEL_FILE}'):\n",
    "    \n",
    "    # Define the XGBoost Classifier with tuned hyperparameters\n",
    "    xgb_classifier = XGBClassifier(eval_metric='logloss',\n",
    "                                learning_rate=0.01,\n",
    "                                max_depth = 3, \n",
    "                                n_estimators = 200, \n",
    "                                subsample = 0.8)\n",
    "\n",
    "    # Train the model\n",
    "    xgb_classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Saving model\n",
    "    joblib.dump(xgb_classifier, f'../models/{MODEL_FILE}')\n",
    "else:\n",
    "    # Loading pre-saved model\n",
    "    xgb_classifier = joblib.load(f\"../models/{MODEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b932419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "xgb_clf_prediction = xgb_classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2369224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for XGBoost Classifier on Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        96\n",
      "           1       0.86      1.00      0.92        61\n",
      "\n",
      "    accuracy                           0.94       157\n",
      "   macro avg       0.93      0.95      0.93       157\n",
      "weighted avg       0.95      0.94      0.94       157\n",
      "\n",
      "============================================================\n",
      "\n",
      "Confusion Matrix for XGBoost Classifier:\n",
      "[[86 10]\n",
      " [ 0 61]]\n",
      "============================================================\n",
      "\n",
      "Validation Accuracy for XGBoost Classifier: 0.9363\n",
      "============================================================\n",
      "\n",
      "XGBoost MSE: 0.0637\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Classification Report for XGBoost Classifier on Validation Set:\")\n",
    "print(classification_report(test_labels, xgb_clf_prediction))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nConfusion Matrix for XGBoost Classifier:\")\n",
    "print(confusion_matrix(test_labels, xgb_clf_prediction))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, xgb_clf_prediction)\n",
    "print(f\"\\nValidation Accuracy for XGBoost Classifier: {accuracy:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_mse = mean_squared_error(test_labels, xgb_clf_prediction)\n",
    "print(f\"\\nXGBoost MSE: {xgb_mse:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f4cbe",
   "metadata": {},
   "source": [
    "#### **I. Executive Summary**\n",
    "---\n",
    "This project successfully developed a high-performance machine learning model to predict the occurrence of a tsunami following a strong earthquake. By analyzing a dataset of 782 seismic events, an XGBoost Classifier was trained and tuned, ultimately achieving **95% accuracy** on unseen test data. The analysis and final model both confirmed that the most critical predictors are not just magnitude, but more specifically the earthquake's **depth**, **year of occurrence**, and **geographic location (offshore vs. land-based)**. This work provides a strong proof-of-concept for an automated tool to aid in early-warning systems.\n",
    "\n",
    "#### **II. Problem Statement**\n",
    "---\n",
    "The objective was to analyze historical earthquake data to identify the key factors that lead to tsunamis and build a reliable classification model to predict the `Tsunami` outcome (1 or 0) for a given seismic event. The primary challenge was to move beyond the general knowledge that \"strong earthquakes cause tsunamis\" and identify more nuanced, predictive patterns in the data.\n",
    "\n",
    "#### **III. Key Findings from Exploratory Data Analysis (EDA)**\n",
    "---\n",
    "The initial EDA was crucial and revealed several key insights that guided the modeling process:\n",
    "* **Imbalance:** The dataset was imbalanced, with only 39% of earthquakes resulting in a tsunami.\n",
    "* **Location is Key:** Tsunami-generating quakes were found to cluster in specific **offshore, oceanic regions near subduction zones**, while non-tsunami events were more broadly distributed.\n",
    "* **Depth Over Magnitude:** While all events in the dataset were strong (magnitude 6.5+), the most significant differentiator was **shallow depth**.\n",
    "* **Temporal Trend:** A clear increase in recorded tsunami-triggering events was observed **after 2012**.\n",
    "\n",
    "#### **IV. Modeling and Evaluation**\n",
    "---\n",
    "1.  **Data Preparation:** A robust **stratified sampling strategy** was employed, creating training, validation, and test sets that preserved the distribution of both the imbalanced `Tsunami` target and the highly skewed `Depth_(km)` feature.\n",
    "2.  **Model Selection:** An **XGBoost Classifier** was chosen for its high performance on tabular data and its ability to capture complex, non-linear interactions between features.\n",
    "3.  **Training:** The final model was trained on a combined set of the training and validation data to maximize learning before the final evaluation.\n",
    "4.  **Performance:** The model's performance on the unseen test set was excellent:\n",
    "    * **Accuracy:** 95.5%\n",
    "    * **F1-Score (Tsunami=1):** 0.93\n",
    "    * **Recall (Tsunami=1):** 0.90 (Correctly identified 90% of all actual tsunamis)\n",
    "    * **Precision (Tsunami=1):** 0.97 (When it predicted a tsunami, it was correct 97% of the time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce9d4e",
   "metadata": {},
   "source": [
    "#### **V. Final Conclusion and Future Work**\n",
    "---\n",
    "This project successfully demonstrates that machine learning can effectively predict tsunami risk from earthquake data. The final XGBoost model is both accurate and reliable, confirming that a combination of an earthquake's depth, location, and recency are stronger predictors than magnitude alone\n",
    "**Future Work:**\n",
    "* **Deployment:** The saved model (`.pkl` file) is ready to be deployed as part of a larger application or API for real-time predictions\n",
    "* **Feature Engineering:** Incorporate additional data, such as distance to the nearest coastline or tectonic plate boundary data, to potentially improve performance further\n",
    "* **Explainability:** Use tools like SHAP (SHapley Additive exPlanations) to provide clear, interpretable reasons for each individual prediction, which is critical for trust in real-world warning systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
